{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalNLP_ass (1).ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeevanlakehead/new/blob/master/FinalNLP_ass_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf9duxZkEU33",
        "colab_type": "code",
        "outputId": "5bbe8e91-3574-4733-885d-c71000f4ed9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "# Import the pandas library to read our dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Get the train/test split package from sklearn for preparing our dataset to\n",
        "# train and test the model with\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import the numpy library to work with and manipulate the data\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import nltk\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM8iAvz5EbWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import data from google drive\n",
        "#data = pd.read_csv('train.tsv', sep='\\t')\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/master/train.tsv', sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBj0PW64FQry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re \n",
        "\n",
        "pd.set_option('max_colwidth',400)\n",
        "\n",
        "wordnet = WordNetLemmatizer()\n",
        "stopwords_en = stopwords.words(\"english\")\n",
        "punctuations = \"?:!.,;-()\"\n",
        "\n",
        "raw_reviews = data.Phrase.values\n",
        "cleaned_reviews = []\n",
        "\n",
        "for i in range(len(raw_reviews)):\n",
        "  review = str(raw_reviews[i])\n",
        "  review=re.sub('[^a-zA-Z]',' ',review)\n",
        "  review=[wordnet.lemmatize(w) for w in word_tokenize(str(review).lower())]\n",
        "  review=' '.join(review)\n",
        "  cleaned_reviews.append(review)\n",
        "\n",
        "data['cleaned_reviews'] = cleaned_reviews\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LvUQpwMFTBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data.cleaned_reviews.values\n",
        "Y = to_categorical(data.Sentiment.values)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=4000,stop_words = None, ngram_range=(1,2))\n",
        "X = vectorizer.fit_transform(X)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2dLEfhjFWXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=2003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLqt3kylFYOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "x_train_np = x_train.toarray()\n",
        "y_train_np = np.array(y_train)\n",
        "\n",
        "x_test_np = x_test.toarray()\n",
        "y_test_np = np.array(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WFO85Y3FaC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.expand_dims(x_train_np, axis=2)\n",
        "x_test = np.expand_dims(x_test_np, axis=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U94z9mguFb5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "from torch.nn import Conv1d\n",
        "from torch.nn import MaxPool1d\n",
        "from torch.nn import Flatten\n",
        "from torch.nn import Linear\n",
        "from torch.nn.functional import relu\n",
        "from torch import sigmoid\n",
        "from torch.utils.data import DataLoader, TensorDataset "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llvXmprNFmqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "def cnn(data_x):\n",
        "  # adding Convolution layer 1\n",
        "  model.add(Conv1D(filters = 64, kernel_size=3, activation='relu', input_shape=(data_x.shape[1],1)))\n",
        "  #adding MaxPool layer\n",
        "  model.add(MaxPooling1D(pool_size =2))\n",
        "  #adding Convolution layer 2\n",
        "  model.add(Conv1D(filters = 128, kernel_size=3, activation='relu'))\n",
        "  #adding MaxPool layer\n",
        "  model.add(MaxPooling1D(pool_size =2))\n",
        "  model.add(Flatten())\n",
        "  #adding Fully connected layer\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  #adding Dropout layer\n",
        "  model.add(Dropout(0.2))\n",
        "  #adding Output Layer\n",
        "  model.add(Dense(5, activation='softmax')) \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GwWqx8kJmpw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import metrics\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVzM61kuKeiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "def recall_k(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_k(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_k(y_true, y_pred):\n",
        "    precision = precision_k(y_true, y_pred)\n",
        "    recall = recall_k(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CaUcnHJFnTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "cnn(x_train_np)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[metrics.categorical_accuracy,f1_k,precision_k, recall_k])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z8GTSfaFpiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(x_train, y_train_np, epochs=5, batch_size=128)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nA8XlhYMXQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, accuracy, f1_score, precision, recall = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Accuracy \" + str(accuracy) + \":\\n\\ff1_score = \" + str(f1_score) +\n",
        "          \"\\nPrecision = \" + str(precision) + \"\\nRecall = \" + str(recall))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmuu4R9rPypo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki9dbNjUP3ac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# load and evaluate a saved model\n",
        "from numpy import loadtxt\n",
        "from keras.models import load_model\n",
        " \n",
        "# load model\n",
        "#model = load_model('model.h5')\n",
        "load_model('model.h5', custom_objects={'f1_m': f1_m, 'precision_m':precision_m, 'recall_m':recall_m})\n",
        "# summarize model.\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}